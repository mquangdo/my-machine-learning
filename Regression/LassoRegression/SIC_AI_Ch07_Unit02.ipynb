{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b54f94",
   "metadata": {
    "id": "84b54f94"
   },
   "source": [
    "## 2.1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "GDflmve01aY1",
   "metadata": {
    "id": "GDflmve01aY1"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "JjQuzQbv168k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1039,
     "status": "ok",
     "timestamp": 1723537587409,
     "user": {
      "displayName": "Minh Khuất Đức",
      "userId": "08890169566633489982"
     },
     "user_tz": -420
    },
    "id": "JjQuzQbv168k",
    "outputId": "f5c5dacb-88e9-4da3-9a31-361b003487e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "DIlt5qNx1g2s",
   "metadata": {
    "id": "DIlt5qNx1g2s"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokeniz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04a14835",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1723537606978,
     "user": {
      "displayName": "Minh Khuất Đức",
      "userId": "08890169566633489982"
     },
     "user_tz": -420
    },
    "id": "04a14835",
    "outputId": "70f44f7b-3443-4417-c9b9-a68fbb8344d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'a', 'cheery', 'as', 'cheery', 'gets', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is a cheery as cheery gets for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ed0a55a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1723537446855,
     "user": {
      "displayName": "Thắng Nguyễn Thị",
      "userId": "12767675916989596521"
     },
     "user_tz": -420
    },
    "id": "7ed0a55a",
    "outputId": "4e23c89b-cb49-4cc9-afb8-e3f32e7309fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'a', 'cheery', 'as', 'cheery', 'gets', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is a cheery as cheery gets for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "723f73ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4081,
     "status": "ok",
     "timestamp": 1723537453605,
     "user": {
      "displayName": "Thắng Nguyễn Thị",
      "userId": "12767675916989596521"
     },
     "user_tz": -420
    },
    "id": "723f73ce",
    "outputId": "621ace1a-daaa-460b-80c6-bd72f5dd28b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'a', 'cheery', 'as', 'cheery', 'gets', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is a cheery as cheery gets for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a06bf16",
   "metadata": {
    "id": "9a06bf16"
   },
   "source": [
    "## 2.2. Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f803492d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1723537884098,
     "user": {
      "displayName": "Minh Khuất Đức",
      "userId": "08890169566633489982"
     },
     "user_tz": -420
    },
    "id": "f803492d",
    "outputId": "13da0609-e001-4894-b44f-5cc5839c9791"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9f012e8-c2bc-4c57-b4e1-64f18783d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed177ee9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1723537974385,
     "user": {
      "displayName": "Minh Khuất Đức",
      "userId": "08890169566633489982"
     },
     "user_tz": -420
    },
    "id": "ed177ee9",
    "outputId": "19273d63-7ec1-4191-e812-487cd140de74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        result.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c2bf4",
   "metadata": {
    "id": "006c2bf4"
   },
   "source": [
    "## 2.3. Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce115299",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1670,
     "status": "ok",
     "timestamp": 1723538143621,
     "user": {
      "displayName": "Minh Khuất Đức",
      "userId": "08890169566633489982"
     },
     "user_tz": -420
    },
    "id": "ce115299",
    "outputId": "390b0d9d-2bd2-4109-b704-8e4c41d6ae49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "n=WordNetLemmatizer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([n.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0849dec4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1723538149620,
     "user": {
      "displayName": "Minh Khuất Đức",
      "userId": "08890169566633489982"
     },
     "user_tz": -420
    },
    "id": "0849dec4",
    "outputId": "2b26d38a-89a7-41bf-fdcf-f91e5b0e53c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a89376b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1723538152416,
     "user": {
      "displayName": "Minh Khuất Đức",
      "userId": "08890169566633489982"
     },
     "user_tz": -420
    },
    "id": "5a89376b",
    "outputId": "5dd921ee-2950-4ebf-87ac-cb34006a4313"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c830548",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1723538153412,
     "user": {
      "displayName": "Minh Khuất Đức",
      "userId": "08890169566633489982"
     },
     "user_tz": -420
    },
    "id": "0c830548",
    "outputId": "022c69fc-1e7f-46c6-9a96-c4df65df9572"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "866080b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1723538155380,
     "user": {
      "displayName": "Minh Khuất Đức",
      "userId": "08890169566633489982"
     },
     "user_tz": -420
    },
    "id": "866080b0",
    "outputId": "4a8f76ac-6fc9-48eb-ce53-86abf95e5181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = PorterStemmer()\n",
    "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "words=word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d22a1b2a",
   "metadata": {
    "id": "d22a1b2a",
    "outputId": "646d7cce-e003-4bda-cbd0-f79288049a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b602500",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1723540277940,
     "user": {
      "displayName": "Minh Khuất Đức",
      "userId": "08890169566633489982"
     },
     "user_tz": -420
    },
    "id": "9b602500",
    "outputId": "e53fa025-051e-454c-ece8-59aea79997ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words=['formalize', 'allowance', 'electricical']\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c110f7a4",
   "metadata": {
    "id": "c110f7a4",
    "outputId": "4f128d49-8a60-4e06-c682-24f3d8b391fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "s = PorterStemmer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5cfff18a",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5cfff18a",
    "outputId": "ee25c5cc-480a-4157-b067-b6cefab5dfe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "l = LancasterStemmer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([l.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70464cd9",
   "metadata": {
    "id": "70464cd9"
   },
   "source": [
    "## 2.4. POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7d344f0c",
   "metadata": {
    "id": "7d344f0c"
   },
   "outputs": [],
   "source": [
    "my_sentence = \"The Colosseum was built by the emperor Vespassian\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b6fca0ce",
   "metadata": {
    "id": "b6fca0ce"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "69330e38",
   "metadata": {
    "id": "69330e38",
    "outputId": "c3a00199-2fd7-4957-fcc5-d4aa0d57b358"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'colosseum', 'was', 'built', 'by', 'the', 'emperor', 'vespassian']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_words = nltk.word_tokenize(my_sentence)\n",
    "for i in range(len(my_words)):\n",
    "    my_words[i] = my_words[i].lower()\n",
    "my_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6139d56b",
   "metadata": {
    "id": "6139d56b",
    "outputId": "f2d6fc5f-fe0c-43b5-ddd6-0f8d5381634f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT'),\n",
       " ('colosseum', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('built', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('emperor', 'NN'),\n",
       " ('vespassian', 'NN')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_words_tagged = nltk.pos_tag(my_words)\n",
    "my_words_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ead6a",
   "metadata": {
    "id": "d49ead6a"
   },
   "source": [
    "## 2.5. Integer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "da77ddb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1723538346647,
     "user": {
      "displayName": "Minh Khuất Đức",
      "userId": "08890169566633489982"
     },
     "user_tz": -420
    },
    "id": "da77ddb2",
    "outputId": "68d95b57-378c-4212-90f1-8cbc12154456"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e37a98f",
   "metadata": {
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1723538344287,
     "user": {
      "displayName": "Minh Khuất Đức",
      "userId": "08890169566633489982"
     },
     "user_tz": -420
    },
    "id": "9e37a98f"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f7d8169c",
   "metadata": {
    "id": "f7d8169c"
   },
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is a good person. a barber is a huge person. he Knew A Secret!. The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secet to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10ecb9c9",
   "metadata": {
    "id": "10ecb9c9",
    "outputId": "dd0b05cc-3b22-4be6-f255-56377381ac8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is a good person.', 'a barber is a huge person.', 'he Knew A Secret!.', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secet to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "text = sent_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "22778660",
   "metadata": {
    "id": "22778660",
    "outputId": "f9bcd492-957a-4f25-a8e8-a29603f20c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secet', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i)\n",
    "    result = []\n",
    "\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words:\n",
    "            if len(word) > 2:\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0\n",
    "                vocab[word] += 1\n",
    "    sentences.append(result)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "65230ed2",
   "metadata": {
    "id": "65230ed2",
    "outputId": "ae4ac2a7-3dbc-4d30-c147-415205d8f8ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 5, 'kept': 4, 'word': 2, 'keeping': 2, 'secet': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "53570e4b",
   "metadata": {
    "id": "53570e4b",
    "outputId": "616d7a6f-df4b-40b4-cf03-9925681c8b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d0120978",
   "metadata": {
    "id": "d0120978",
    "outputId": "d8c0ddfd-6c0e-49cb-9c6d-1bf971d94a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('huge', 5), ('secret', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('secet', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fcf9712b",
   "metadata": {
    "id": "fcf9712b",
    "outputId": "1a0a403c-bd48-42b0-feec-06881193d71c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'huge': 2, 'secret': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab_sorted:\n",
    "    if frequency > 1:\n",
    "        i=i+1\n",
    "        word_to_index[word]=i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1fc266c4",
   "metadata": {
    "id": "1fc266c4",
    "outputId": "64892170-351c-4ce1-913e-07ae786cc0a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'huge': 2, 'secret': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "word_frequency = [w for w, c in word_to_index.items() if c >= vocab_size + 1]\n",
    "for w in word_frequency:\n",
    "    del word_to_index[w]\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "10916adf",
   "metadata": {
    "id": "10916adf"
   },
   "outputs": [],
   "source": [
    "word_to_index['OOV'] = len(word_to_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "48dedbf5",
   "metadata": {
    "id": "48dedbf5",
    "outputId": "0a410dd2-2537-4d7a-f9d1-f0a274625bd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 5], [1, 2, 5], [6, 3], [3, 4, 2, 3], [2, 3], [1, 4, 6], [1, 4, 6], [1, 4, 3], [6, 6, 2, 6, 6, 1, 6], [1, 6, 2, 6]]\n"
     ]
    }
   ],
   "source": [
    "encoded = []\n",
    "for s in sentences:\n",
    "    temp = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp.append(word_to_index[w])\n",
    "        except KeyError:\n",
    "            temp.append(word_to_index['OOV'])\n",
    "    encoded.append(temp)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ffc82d77",
   "metadata": {
    "id": "ffc82d77"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6d008a0c",
   "metadata": {
    "id": "6d008a0c",
    "outputId": "73244355-e6c8-4ccb-edf7-7e3a665cdac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secet', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "149faf69",
   "metadata": {
    "id": "149faf69",
    "outputId": "705ceb11-2e4a-4696-f41a-f0e0af6b5777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secet', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "words = sum(sentences, [])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "719e8898",
   "metadata": {
    "id": "719e8898",
    "outputId": "2c3137c9-71ec-4b9a-cf2a-fb354da8caea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'huge': 5, 'secret': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'secet': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter(words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "06d05796",
   "metadata": {
    "id": "06d05796",
    "outputId": "9277d072-9d99-4628-c3de-8548fbcd6209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2020bde8",
   "metadata": {
    "id": "2020bde8",
    "outputId": "275a52ac-2240-430c-e7e1-23489da62ab9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('huge', 5), ('secret', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cbc62680",
   "metadata": {
    "id": "cbc62680",
    "outputId": "17fa210a-7f18-49b6-cf94-51b4c43c7298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'huge': 2, 'secret': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab:\n",
    "    i=i+1\n",
    "    word_to_index[word]=i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bcfb8746",
   "metadata": {
    "id": "bcfb8746"
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a497fc8d",
   "metadata": {
    "id": "a497fc8d"
   },
   "outputs": [],
   "source": [
    "vocab = FreqDist(np.hstack(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bf53e1b5",
   "metadata": {
    "id": "bf53e1b5",
    "outputId": "67c03d39-7c1b-4305-8414-930086e73813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8c70ba89",
   "metadata": {
    "id": "8c70ba89",
    "outputId": "2a3127b6-324a-4fd2-a67b-8425129647fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('huge', 5), ('secret', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ff8e09aa",
   "metadata": {
    "id": "ff8e09aa",
    "outputId": "e258c229-7d3e-4d8a-fc9a-671ea08b2bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'huge': 2, 'secret': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1666d67b",
   "metadata": {
    "id": "1666d67b",
    "outputId": "3e81c8f8-4041-4abc-e43a-6ca1e6abcd82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : a, index : 0\n",
      "value : b, index : 1\n",
      "value : c, index : 2\n",
      "value : d, index : 3\n",
      "value : e, index : 4\n"
     ]
    }
   ],
   "source": [
    "test=['a', 'b', 'c', 'd', 'e']\n",
    "for index, value in enumerate(test):\n",
    "    print(\"value : {}, index : {}\".format(value, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6f05bc23",
   "metadata": {
    "id": "6f05bc23"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ca8f2a8c",
   "metadata": {
    "id": "ca8f2a8c"
   },
   "outputs": [],
   "source": [
    "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "cf1c1229",
   "metadata": {
    "id": "cf1c1229"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0fd1840b",
   "metadata": {
    "id": "0fd1840b",
    "outputId": "31dc63d2-b46c-4867-f39e-bd28811cd3b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d2c5b6cf",
   "metadata": {
    "id": "d2c5b6cf",
    "outputId": "a877d6b2-7c3a-4a37-b39a-8e888d08e37c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2b1f8fd4",
   "metadata": {
    "id": "2b1f8fd4",
    "outputId": "940d09c0-b3d6-4f34-fd28-6ec8e5f9db4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba708a",
   "metadata": {
    "id": "eaba708a"
   },
   "source": [
    "## 2.6. Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0314b196",
   "metadata": {
    "id": "0314b196"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "514b237e",
   "metadata": {
    "id": "514b237e"
   },
   "outputs": [],
   "source": [
    "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'],\n",
    "           ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'],\n",
    "           ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'],\n",
    "           ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
    "           ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "cd308ecf",
   "metadata": {
    "id": "cd308ecf"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d888b8b0",
   "metadata": {
    "id": "d888b8b0",
    "outputId": "25a00cf2-5ab3-4caf-f85f-6defd18094a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3b67904c",
   "metadata": {
    "id": "3b67904c",
    "outputId": "ae7996ab-57dd-461c-e51a-a7cd0e11c360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(item) for item in encoded)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5ae40fa0",
   "metadata": {
    "id": "5ae40fa0",
    "outputId": "3efb0818-d21f-4297-8320-9cdd632d020e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0,  0,  0],\n",
       "       [ 3,  2,  0,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0,  0,  0]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for item in encoded:\n",
    "    while len(item) < max_len:\n",
    "        item.append(0)\n",
    "\n",
    "padded_np = np.array(encoded)\n",
    "padded_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "efca08b7",
   "metadata": {
    "id": "efca08b7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "95a4b5dc",
   "metadata": {
    "id": "95a4b5dc",
    "outputId": "9c530fc9-b0d9-41d0-8334-780d7e32130e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3b174286",
   "metadata": {
    "id": "3b174286",
    "outputId": "def2f3bd-b159-41e8-bf82-bc52df5ee3f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  1,  5],\n",
       "       [ 0,  0,  0,  0,  1,  8,  5],\n",
       "       [ 0,  0,  0,  0,  1,  3,  5],\n",
       "       [ 0,  0,  0,  0,  0,  9,  2],\n",
       "       [ 0,  0,  0,  2,  4,  3,  2],\n",
       "       [ 0,  0,  0,  0,  0,  3,  2],\n",
       "       [ 0,  0,  0,  0,  1,  4,  6],\n",
       "       [ 0,  0,  0,  0,  1,  4,  6],\n",
       "       [ 0,  0,  0,  0,  1,  4,  2],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 0,  0,  0,  1, 12,  3, 13]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f1ad3",
   "metadata": {
    "id": "b87f1ad3"
   },
   "source": [
    "## 2.7. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "aeac4706",
   "metadata": {
    "id": "aeac4706"
   },
   "outputs": [],
   "source": [
    "text='I want to go to lunch with me. The lunch menu is hamburgers. Hamburgers are the best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "37be72e6",
   "metadata": {
    "id": "37be72e6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4d857d88",
   "metadata": {
    "id": "4d857d88",
    "outputId": "f32d9c9f-69ff-4df1-e921-d146865a5ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 1, 'lunch': 2, 'the': 3, 'hamburgers': 4, 'i': 5, 'want': 6, 'go': 7, 'with': 8, 'me': 9, 'menu': 10, 'is': 11, 'are': 12, 'best': 13}\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "39c9ccc1",
   "metadata": {
    "id": "39c9ccc1",
    "outputId": "418e495c-05c7-4359-fe28-153f35daebaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 6, 1, 7, 1, 2, 8, 9, 3, 2, 10, 11, 4, 4, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "encoded=t.texts_to_sequences([text])\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d2ae3fba",
   "metadata": {
    "id": "d2ae3fba",
    "outputId": "2371ba99-4636-4642-ff9d-79606470278b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "one_hot = to_categorical(encoded)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56346d4-59f6-4026-a6f9-277b90319947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
